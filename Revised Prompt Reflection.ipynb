{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c86acc2-f0e3-4d9c-863b-b041296b1b5d",
   "metadata": {},
   "source": [
    "# Reflection on Tutoring Prompt Revision\n",
    "\n",
    "---\n",
    "\n",
    "These are the changes I wanted to incorporate in my revisions:\n",
    "1. make the tutor more stern who keeps the student accountable to learning\n",
    "2. make the tutor drive the conversation even after the initial topic is explored / make the tutor's teaching approach consistent when the student asks another question\n",
    "3. make the tutor incorporate student learning material\n",
    "\n",
    "I would say that they were somewhat successful.\n",
    "\n",
    "---\n",
    "### Stern Tutor\n",
    "\n",
    "I felt that both of the revised prompt chats, the LLM was more prepared to combat my distraction techniques. For example, in response to my irrelevant questions it said:\n",
    "> Staying focused on the cardiac cycle will help us make the most of our time together. If you're ready, we can move on to the final phase – **Ventricular Diastole** – where the ventricles relax. Would you like to dive into that next? (revised session #2)\n",
    "\n",
    "However, I can attest that the LLM's ability to stay on track is not fool-proof. Eventually, we ended up talking about the history of hats. Gpt-4o was also prone to rearing off-topic in related topics. To clarify, it went on tangents on slightly overlapping material without realizing it's going off topic from the original learning objective. I did this by asking the LLM to explain Wilhelm His Jr., a cardiologist who discovered the bundle of his, which is part of the cardiac conduction system which fuels the cardiac cycle. While somewhat related, this is not knowledge that is directly related to the topic, the cardiac cycle. So the LLM unintentionally went off topic by explaining this to me.\n",
    "\n",
    "--- \n",
    "### Tutor consistently drives conversation\n",
    "\n",
    "This effort was largely unsuccessful. This problem persisted between session #2, and both the revised sessions #1 and #2. The open-ended questions that the LLM provided at the end of explanation almost always defaulted to something along the lines of, \"If you're curious about more details or would like further explanations... let me know!\" (revised session #1 and #2). So, while the LLM did ask a question to \"drive conversation,\" it was not specific enough to consistently maintain a conversation.\n",
    "\n",
    "---\n",
    "### Student learning material\n",
    "\n",
    "I sent gpt-4o a website link with my study topic for the purpose of this chat experiment to simulate my class material.\n",
    "> https://open.oregonstate.education/anatomy2e/chapter/cardiac-cycle/\n",
    "\n",
    "I thought that incorporating this was particularly helpful in helping the LLM frame the initial part of the tutoring session where the tutor lays an outline of what they will learn. However, it is possible that I was unable to completely simulate having student material as the source I used was from the internet, which gpt-4o may have already been accessing for previous conversations. I consider this revision to be successful to a limited extent for this reason as the material was explored further, but I am also unable to direct this success due to this specific change. However, I do believe that this is a desirable tutor trait.\n",
    "\n",
    "--- \n",
    "\n",
    "### Revised session #1 vs #2\n",
    "\n",
    "Overall, there was not much of a difference between the two revised prompts--the difference being that one includes the wrap up step (session #1) and the other doesn't (session #2). I was expecting to see that session #2 would be able to continue its open-ended question formmating for longer because it no longer had a \"conclusion\" piece incoporated into its structured prompting, however, the LLM appeared to naturally find a way to end converation by asking if the student showed understanding of the topic. In this manner, omitting it doesn't seem to make a big of a difference. However, the chat seems to have a cleaner ending if the LLM is given this prompting aspect as denoted in:\n",
    "> I'm glad to hear that made things clearer for you!... If you have any more questions about this topic or if there's anything else you'd like to learn about, feel free to let me know.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
